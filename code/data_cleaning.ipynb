{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning & Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lyme Disease by State data\n",
    "\n",
    "Since this data only came with state specification, but the climatic variables were only available by Region, I had to manually map each state to one of the four U.S. regions (Northeastern/Eastern, Central/Midwest, Southern, and Western) to which it belongs. In the cell below, I create the mapping function and apply it to the dataset, as well as other general cleaning tasks to ensure compatible data with other sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read file with latin1 encoding\n",
      "\n",
      "First few rows of the cleaned dataset:\n",
      "        State  Year  Lyme_cases   region\n",
      "13   Illinois  2008         108  central\n",
      "65   Illinois  2009         136  central\n",
      "117  Illinois  2010         135  central\n",
      "169  Illinois  2011         194  central\n",
      "221  Illinois  2012         204  central\n",
      "\n",
      "Unique state names after cleaning:\n",
      "['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware', 'District of Columbia', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'US Total', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming']\n",
      "\n",
      "Summary of data by region:\n",
      "          Number of States\n",
      "region                    \n",
      "central                 12\n",
      "eastern                 12\n",
      "southern                12\n",
      "western                 13\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_region_mapping():\n",
    "    return {\n",
    "        # Northeast/Eastern Region\n",
    "        'Maine': 'eastern',\n",
    "        'New Hampshire': 'eastern',\n",
    "        'Vermont': 'eastern',\n",
    "        'Massachusetts': 'eastern',\n",
    "        'Rhode Island': 'eastern',\n",
    "        'Connecticut': 'eastern',\n",
    "        'New York': 'eastern',\n",
    "        'Pennsylvania': 'eastern',\n",
    "        'New Jersey': 'eastern',\n",
    "        'Delaware': 'eastern',\n",
    "        'Maryland': 'eastern',\n",
    "        'District of Columbia': 'eastern',\n",
    "        \n",
    "        # Southern Region\n",
    "        'Virginia': 'southern',\n",
    "        'West Virginia': 'southern',\n",
    "        'Kentucky': 'southern',\n",
    "        'Tennessee': 'southern',\n",
    "        'North Carolina': 'southern',\n",
    "        'South Carolina': 'southern',\n",
    "        'Georgia': 'southern',\n",
    "        'Florida': 'southern',\n",
    "        'Alabama': 'southern',\n",
    "        'Mississippi': 'southern',\n",
    "        'Louisiana': 'southern',\n",
    "        'Arkansas': 'southern',\n",
    "        \n",
    "        # Central Region\n",
    "        'Ohio': 'central',\n",
    "        'Indiana': 'central',\n",
    "        'Illinois': 'central',\n",
    "        'Michigan': 'central',\n",
    "        'Wisconsin': 'central',\n",
    "        'Minnesota': 'central',\n",
    "        'Iowa': 'central',\n",
    "        'Missouri': 'central',\n",
    "        'North Dakota': 'central',\n",
    "        'South Dakota': 'central',\n",
    "        'Nebraska': 'central',\n",
    "        'Kansas': 'central',\n",
    "        \n",
    "        # Western Region\n",
    "        'Montana': 'western',\n",
    "        'Idaho': 'western',\n",
    "        'Wyoming': 'western',\n",
    "        'Colorado': 'western',\n",
    "        'New Mexico': 'western',\n",
    "        'Arizona': 'western',\n",
    "        'Utah': 'western',\n",
    "        'Nevada': 'western',\n",
    "        'California': 'western',\n",
    "        'Oregon': 'western',\n",
    "        'Washington': 'western',\n",
    "        'Alaska': 'western',\n",
    "        'Hawaii': 'western'\n",
    "    }\n",
    "\n",
    "# apply mapping, clean state names, and melt the dataframe \n",
    "def clean_lyme_data(df):\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Clean state names --> remove unusual characters (the original dataset randomly \n",
    "    # had some weird characters appended to the end of the state name)\n",
    "    df_clean['State'] = df_clean['State'].str.replace(r'[^a-zA-Z\\s]', '', regex=True)\n",
    "    \n",
    "    # melt the dataframe --> convert years to rows\n",
    "    df_melted = pd.melt(\n",
    "        df_clean,\n",
    "        id_vars=['State'],\n",
    "        var_name='Year',\n",
    "        value_name='Lyme_cases'\n",
    "    )\n",
    "    \n",
    "    # region mapping\n",
    "    region_mapping = create_region_mapping()\n",
    "    \n",
    "    # add region column\n",
    "    df_melted['region'] = df_melted['State'].map(region_mapping)\n",
    "    \n",
    "    # convert Year to integer\n",
    "    df_melted['Year'] = pd.to_numeric(df_melted['Year'])\n",
    "    \n",
    "    # sort by region, state, and year\n",
    "    df_melted = df_melted.sort_values(['region', 'State', 'Year'])\n",
    "\n",
    "    df_melted['Lyme_cases'] = df_melted['Lyme_cases'].str.replace(',', '').astype(int)\n",
    "    \n",
    "    return df_melted\n",
    "\n",
    "df = None\n",
    "\n",
    "# try different encodings until one works (to remove the weird characters)\n",
    "encodings_to_try = ['latin1', 'cp1252', 'iso-8859-1', 'utf-8']\n",
    "for encoding in encodings_to_try:\n",
    "    try:\n",
    "        df = pd.read_csv('../data/raw_data/lyme_states_2008-2022_WIDE.csv', encoding=encoding)\n",
    "        print(f\"Successfully read file with {encoding} encoding\")\n",
    "        break\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Failed with {encoding} encoding\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"Different error with {encoding} encoding: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# apply cleaning function to the data\n",
    "state_lyme = clean_lyme_data(df)\n",
    "\n",
    "print(\"\\nFirst few rows of the cleaned dataset:\")\n",
    "print(state_lyme.head())\n",
    "\n",
    "print(\"\\nUnique state names after cleaning:\")\n",
    "print(sorted(state_lyme['State'].unique()))\n",
    "\n",
    "print(\"\\nSummary of data by region:\")\n",
    "print(state_lyme.groupby('region')['State'].nunique().to_frame('Number of States'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Climate data\n",
    "\n",
    "For this data, I had to download 12 CSV files -- one for each of the three climatic variables of interest (average temperature, minimum temperature, and average precipitation) for each of the four regions. To clean this data, I wrote function for each climatic variable that could loop over each of the regional datasets for that variable. I had to make sure when I was downloading the datasets to name the files consistently. All files began with the region name in lowercase (southern, eastern, central, western) followed by an underscore and then the climatic variable, denoted as either _avgtemp.csv, _mintemp.csv, or _precipitation.csv. This naming consistency allowed me to loop over all files with similar endings, and also extract the region name before the underscore and add it as a column in order to identify the data's region when merged with the other data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing average temperature data...\n",
      "../data/raw_data/western_avgtemp.csv\n",
      "../data/raw_data/central_avgtemp.csv\n",
      "../data/raw_data/eastern_avgtemp.csv\n",
      "../data/raw_data/southern_avgtemp.csv\n",
      "\n",
      "Processing ../data/raw_data/western_avgtemp.csv\n",
      "Extracted region: western\n",
      "Read 21 rows from file\n",
      "\n",
      "Processing ../data/raw_data/central_avgtemp.csv\n",
      "Extracted region: central\n",
      "Read 21 rows from file\n",
      "\n",
      "Processing ../data/raw_data/eastern_avgtemp.csv\n",
      "Extracted region: eastern\n",
      "Read 21 rows from file\n",
      "\n",
      "Processing ../data/raw_data/southern_avgtemp.csv\n",
      "Extracted region: southern\n",
      "Read 21 rows from file\n",
      "\n",
      "Processing minimum temperature data...\n",
      "../data/raw_data/western_mintemp.csv\n",
      "../data/raw_data/southern_mintemp.csv\n",
      "../data/raw_data/eastern_mintemp.csv\n",
      "../data/raw_data/central_mintemp.csv\n",
      "\n",
      "Processing ../data/raw_data/western_mintemp.csv\n",
      "Extracted region: western\n",
      "Read 15 rows from file\n",
      "\n",
      "Processing ../data/raw_data/southern_mintemp.csv\n",
      "Extracted region: southern\n",
      "Read 15 rows from file\n",
      "\n",
      "Processing ../data/raw_data/eastern_mintemp.csv\n",
      "Extracted region: eastern\n",
      "Read 15 rows from file\n",
      "\n",
      "Processing ../data/raw_data/central_mintemp.csv\n",
      "Extracted region: central\n",
      "Read 15 rows from file\n",
      "\n",
      "Processing processing data...\n",
      "../data/raw_data/western_precipitation.csv\n",
      "../data/raw_data/central_precipitation.csv\n",
      "../data/raw_data/eastern_precipitation.csv\n",
      "../data/raw_data/southern_precipitation.csv\n",
      "\n",
      "Processing ../data/raw_data/western_precipitation.csv\n",
      "Extracted region: western\n",
      "Read 15 rows from file\n",
      "\n",
      "Processing ../data/raw_data/central_precipitation.csv\n",
      "Extracted region: central\n",
      "Read 15 rows from file\n",
      "\n",
      "Processing ../data/raw_data/eastern_precipitation.csv\n",
      "Extracted region: eastern\n",
      "Read 15 rows from file\n",
      "\n",
      "Processing ../data/raw_data/southern_precipitation.csv\n",
      "Extracted region: southern\n",
      "Read 15 rows from file\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# function to clean the Average Temperature data \n",
    "def clean_avgtemp_data():\n",
    "    \n",
    "    avg_dfs = []\n",
    "    \n",
    "    # find all avgtemp data for each region\n",
    "    avgtemp_files = glob.glob(\"../data/raw_data/*avgtemp.csv\")  \n",
    "    for file in avgtemp_files:\n",
    "        print(file)\n",
    "    \n",
    "    # Process each avgtemp file for the regions\n",
    "    for file_path in avgtemp_files:\n",
    "        print(f\"\\nProcessing {file_path}\")\n",
    "        # Extract region from filename\n",
    "        region = os.path.basename(file_path).split('_')[0]\n",
    "        print(f\"Extracted region: {region}\")\n",
    "        \n",
    "        try:\n",
    "            # Read CSV file (skip first 4 rows, which contain descriptive information)\n",
    "            df = pd.read_csv(file_path, skiprows=4)\n",
    "            print(f\"Read {len(df)} rows from file\")\n",
    "            \n",
    "            # Clean and rename necessary columns\n",
    "            df = df[['Date', 'Value']] \n",
    "            df = df.rename(columns={\n",
    "                'Date': 'Year',\n",
    "                'Value': 'Avg_temp'\n",
    "            })\n",
    "            \n",
    "            # extract only year from the \"date\" column \n",
    "            df['Year'] = df['Year'].astype(str).str[:4].astype(int)\n",
    "            \n",
    "            # add region column\n",
    "            df['region'] = region\n",
    "            \n",
    "            avg_dfs.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "    # Combine all regions\n",
    "    if avg_dfs:\n",
    "        return pd.concat(avg_dfs, ignore_index=True)\n",
    "    return None\n",
    "\n",
    "\n",
    "# Function to clean minimum temperature data \n",
    "def clean_mintemp_data():\n",
    "    min_dfs = []\n",
    "    \n",
    "    # find the mintemp files for each region \n",
    "    mintemp_files = glob.glob(\"../data/raw_data/*mintemp.csv\") \n",
    "    for file in mintemp_files:\n",
    "        print(file)\n",
    "    \n",
    "    # Process each mintemp file for the regions\n",
    "    for file_path in mintemp_files:\n",
    "        print(f\"\\nProcessing {file_path}\")\n",
    "        # Extract region from filename (I had to specify the region name when I downloaded and saved the data)\n",
    "        region = os.path.basename(file_path).split('_')[0]\n",
    "        print(f\"Extracted region: {region}\")\n",
    "        \n",
    "        try:\n",
    "            # Read CSV file (skip the first 4 rows)\n",
    "            df = pd.read_csv(file_path, skiprows=4)\n",
    "            print(f\"Read {len(df)} rows from file\")\n",
    "            \n",
    "            # Clean and rename necessary columns\n",
    "            df = df[['Date', 'Value']] \n",
    "            df = df.rename(columns={\n",
    "                'Date': 'Year',\n",
    "                'Value': 'Min_temp_avg'\n",
    "            })\n",
    "            \n",
    "            # Extract year from date column\n",
    "            df['Year'] = df['Year'].astype(str).str[:4].astype(int)\n",
    "            \n",
    "            # Add region column\n",
    "            df['region'] = region\n",
    "            \n",
    "            min_dfs.append(df)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "    # Combine all regions\n",
    "    if min_dfs:\n",
    "        return pd.concat(min_dfs, ignore_index=True)\n",
    "    return None\n",
    "\n",
    "\n",
    "# Function to clea precipitation data\n",
    "def clean_precipitation_data():\n",
    "    # List to store dataframes\n",
    "    prec_dfs = []\n",
    "    \n",
    "    # Show which files we're finding\n",
    "    precipitation_files = glob.glob(\"../data/raw_data/*precipitation.csv\")  # Removed underscore from pattern\n",
    "    for file in precipitation_files:\n",
    "        print(file)\n",
    "    \n",
    "    # Process each mintemp file for the regions\n",
    "    for file_path in precipitation_files:\n",
    "        print(f\"\\nProcessing {file_path}\")\n",
    "        # Extract region from filename\n",
    "        region = os.path.basename(file_path).split('_')[0]\n",
    "        print(f\"Extracted region: {region}\")\n",
    "        \n",
    "        try:\n",
    "            # Read CSV file, skipping the first 4 rows\n",
    "            df = pd.read_csv(file_path, skiprows=4)\n",
    "            print(f\"Read {len(df)} rows from file\")\n",
    "            \n",
    "            # Clean and rename columns\n",
    "            df = df[['Date', 'Value']]  # Keep only needed columns\n",
    "            df = df.rename(columns={\n",
    "                'Date': 'Year',\n",
    "                'Value': 'Precipitation_avg'\n",
    "            })\n",
    "            \n",
    "            # Extract year from the date column\n",
    "            df['Year'] = df['Year'].astype(str).str[:4].astype(int)\n",
    "            \n",
    "            # Add region column\n",
    "            df['region'] = region\n",
    "            \n",
    "            prec_dfs.append(df)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "    # Combine all regions\n",
    "    if prec_dfs:\n",
    "        return pd.concat(prec_dfs, ignore_index=True)\n",
    "    return None\n",
    "\n",
    "# Process both types of data\n",
    "print(\"Processing average temperature data...\")\n",
    "avg_temp_data = clean_avgtemp_data()\n",
    "print(\"\\nProcessing minimum temperature data...\")\n",
    "min_temp_data = clean_mintemp_data()\n",
    "print(\"\\nProcessing processing data...\")\n",
    "precipitation_data = clean_precipitation_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree coverage loss data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        State  Total_Land_Area  Year  Tree_Cover_Loss\n",
      "0     Alabama         13363464  2001           168587\n",
      "1      Alaska        150737804  2001            27964\n",
      "2     Arizona         29535713  2001              653\n",
      "3    Arkansas         13769059  2001           110114\n",
      "4  California         40961694  2001            39102\n"
     ]
    }
   ],
   "source": [
    "tcloss = pd.read_csv('../data/raw_data/treecoverlossdata.csv')\n",
    "\n",
    "# filter for threshold = 75 and then remove the threshold column\n",
    "filtered_data = tcloss[tcloss['threshold'] == 75]\n",
    "\n",
    "# remove other unnecessary columns\n",
    "columns_to_remove = ['country', 'extent_2000_ha', 'extent_2010_ha', 'gain_2000-2020_ha', 'threshold']\n",
    "filtered_data = filtered_data.drop(columns=columns_to_remove)\n",
    "\n",
    "# melt on tc_loss_ha_20** columns\n",
    "melted_tc = pd.melt(\n",
    "    filtered_data,\n",
    "    id_vars=['subnational1', 'area_ha'],\n",
    "    value_vars=[col for col in filtered_data.columns if col.startswith('tc_loss_ha_')],\n",
    "    var_name='Year',\n",
    "    value_name='Tree_Cover_Loss'\n",
    ")\n",
    "\n",
    "# clean the 'Year' column to retain only the numeric year\n",
    "melted_tc['Year'] = melted_tc['Year'].str.extract(r'(\\d{4})')\n",
    "melted_tc['Year'] = pd.to_numeric(melted_tc['Year'])\n",
    "\n",
    "# rename columns\n",
    "melted_tc = melted_tc.rename(columns={'subnational1': 'State', 'area_ha': 'Total_Land_Area'})\n",
    "\n",
    "print(melted_tc.head()) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Species richness data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        State  species_richness  state_park_land_coverage  state_park_rank\n",
      "0     Alabama              10.0                    0.0023               46\n",
      "1      Alaska               NaN                    0.0910                2\n",
      "2     Arizona               5.0                    0.0260               12\n",
      "3    Arkansas               8.0                    0.0018               44\n",
      "4  California              10.0                    0.0749                3\n"
     ]
    }
   ],
   "source": [
    "species = pd.read_csv('../data/raw_data/species_richness_by_state.csv')\n",
    "species = species.rename(columns={'state': 'State'})\n",
    "\n",
    "# convert the percentage input to numeric \n",
    "species['state_park_land_coverage'] = species['state_park_land_coverage'].str.replace('%', '')\n",
    "species['state_park_land_coverage'] = pd.to_numeric(species['state_park_land_coverage']) / 100\n",
    "\n",
    "print(species.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year  Precipitation_avg   region  Min_temp_avg  Avg_temp     State  \\\n",
      "0  2008               4.49  central          53.8      66.1  Illinois   \n",
      "1  2008               4.49  central          53.8      66.1   Indiana   \n",
      "2  2008               4.49  central          53.8      66.1      Iowa   \n",
      "3  2008               4.49  central          53.8      66.1    Kansas   \n",
      "4  2008               4.49  central          53.8      66.1  Michigan   \n",
      "\n",
      "   Lyme_cases  Total_Land_Area  Tree_Cover_Loss  species_richness  \\\n",
      "0         108         15008781              615               6.0   \n",
      "1          42          9436269             1051               8.0   \n",
      "2         109         14584483              209               5.0   \n",
      "3          16         21312413              460               1.0   \n",
      "4          92         25036039            27919               3.0   \n",
      "\n",
      "   state_park_land_coverage  state_park_rank  \n",
      "0                    0.0139             19.0  \n",
      "1                    0.0081             32.0  \n",
      "2                    0.0021             42.0  \n",
      "3                    0.0006             44.0  \n",
      "4                    0.0223             11.0  \n"
     ]
    }
   ],
   "source": [
    "merged = min_temp_data.merge(avg_temp_data, on=['Year', 'region'], how='left')\n",
    "merged2 = precipitation_data.merge(merged, on=['Year', 'region'], how='outer')\n",
    "merged3 = merged2.merge(state_lyme, on=['Year', 'region'], how='left')\n",
    "merged4 = merged3.merge(melted_tc, on=['Year', 'State'], how=\"left\")\n",
    "merged5 = merged4.merge(species, on='State', how='left')\n",
    "print(merged5.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Year                          int64\n",
       "Precipitation_avg           float64\n",
       "region                       object\n",
       "Min_temp_avg                float64\n",
       "Avg_temp                    float64\n",
       "State                        object\n",
       "Lyme_cases                    int64\n",
       "Total_Land_Area               int64\n",
       "Tree_Cover_Loss               int64\n",
       "species_richness            float64\n",
       "state_park_land_coverage    float64\n",
       "state_park_rank             float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged5.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged5.to_csv('../data/clean_data/data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning RCP 8.5 future climate scenarios data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     region  Avg_temp  Min_temp  Precipitation_avg  Total_Land_Area  \\\n",
      "0   central     79.33     67.81              12.41       3198039870   \n",
      "1   eastern     72.39     61.21              12.76        710057100   \n",
      "2  southern     86.36     75.29              12.58       2105379510   \n",
      "3   western     74.49     59.00               7.50       6895077150   \n",
      "\n",
      "   state_park_land_coverage  state_park_rank  \n",
      "0                  0.008292        31.083333  \n",
      "1                  0.027827        17.363636  \n",
      "2                  0.012292        30.916667  \n",
      "3                  0.032108        19.923077  \n"
     ]
    }
   ],
   "source": [
    "# load the manually compiled dataset\n",
    "rcp = pd.read_csv('../data/raw_data/RCP8.5_data.csv')\n",
    "\n",
    "# pivot wider so that each indicator is its own column\n",
    "rcp = rcp.pivot(index='region', columns='Indicator', values='value').reset_index()\n",
    "\n",
    "# extract region aggregates from the original dataset\n",
    "region_aggregates = merged5.groupby('region').agg({\n",
    "    'Total_Land_Area': 'sum',  # Sum for total land area\n",
    "    'state_park_land_coverage': 'mean',  # Average for park land coverage\n",
    "    'state_park_rank': 'mean'  # Average for park rank\n",
    "}).reset_index()\n",
    "\n",
    "# merge\n",
    "future_data = pd.merge(rcp, region_aggregates, on='region')\n",
    "\n",
    "print(future_data)\n",
    "\n",
    "future_data.to_csv('../data/clean_data/2050inputs.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
