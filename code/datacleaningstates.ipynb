{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:70: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:70: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/var/folders/0s/3s0t3s4d31d4xtm_jt4pfhpr0000gn/T/ipykernel_38997/967449051.py:70: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  df_clean['State'] = df_clean['State'].str.replace('[^a-zA-Z\\s]', '', regex=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying latin1 encoding...\n",
      "Successfully read file with latin1 encoding\n",
      "\n",
      "First few rows of the cleaned dataset:\n",
      "        State  Year  Lyme_cases   region\n",
      "13   Illinois  2008         108  central\n",
      "65   Illinois  2009         136  central\n",
      "117  Illinois  2010         135  central\n",
      "169  Illinois  2011         194  central\n",
      "221  Illinois  2012         204  central\n",
      "\n",
      "Unique state names after cleaning:\n",
      "['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware', 'District of Columbia', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'US Total', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming']\n",
      "\n",
      "Summary of data by region:\n",
      "          Number of States\n",
      "region                    \n",
      "central                 12\n",
      "eastern                 12\n",
      "southern                12\n",
      "western                 13\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_region_mapping():\n",
    "    \"\"\"Create dictionary mapping states to their regions\"\"\"\n",
    "    return {\n",
    "        # [Your existing region mapping dictionary stays the same]\n",
    "        # Northeast/Eastern Region\n",
    "        'Maine': 'eastern',\n",
    "        'New Hampshire': 'eastern',\n",
    "        'Vermont': 'eastern',\n",
    "        'Massachusetts': 'eastern',\n",
    "        'Rhode Island': 'eastern',\n",
    "        'Connecticut': 'eastern',\n",
    "        'New York': 'eastern',\n",
    "        'Pennsylvania': 'eastern',\n",
    "        'New Jersey': 'eastern',\n",
    "        'Delaware': 'eastern',\n",
    "        'Maryland': 'eastern',\n",
    "        'District of Columbia': 'eastern',\n",
    "        \n",
    "        # Southern Region\n",
    "        'Virginia': 'southern',\n",
    "        'West Virginia': 'southern',\n",
    "        'Kentucky': 'southern',\n",
    "        'Tennessee': 'southern',\n",
    "        'North Carolina': 'southern',\n",
    "        'South Carolina': 'southern',\n",
    "        'Georgia': 'southern',\n",
    "        'Florida': 'southern',\n",
    "        'Alabama': 'southern',\n",
    "        'Mississippi': 'southern',\n",
    "        'Louisiana': 'southern',\n",
    "        'Arkansas': 'southern',\n",
    "        \n",
    "        # Central Region\n",
    "        'Ohio': 'central',\n",
    "        'Indiana': 'central',\n",
    "        'Illinois': 'central',\n",
    "        'Michigan': 'central',\n",
    "        'Wisconsin': 'central',\n",
    "        'Minnesota': 'central',\n",
    "        'Iowa': 'central',\n",
    "        'Missouri': 'central',\n",
    "        'North Dakota': 'central',\n",
    "        'South Dakota': 'central',\n",
    "        'Nebraska': 'central',\n",
    "        'Kansas': 'central',\n",
    "        \n",
    "        # Western Region\n",
    "        'Montana': 'western',\n",
    "        'Idaho': 'western',\n",
    "        'Wyoming': 'western',\n",
    "        'Colorado': 'western',\n",
    "        'New Mexico': 'western',\n",
    "        'Arizona': 'western',\n",
    "        'Utah': 'western',\n",
    "        'Nevada': 'western',\n",
    "        'California': 'western',\n",
    "        'Oregon': 'western',\n",
    "        'Washington': 'western',\n",
    "        'Alaska': 'western',\n",
    "        'Hawaii': 'western'\n",
    "    }\n",
    "\n",
    "def clean_lyme_data(df):\n",
    "    # Create copy of dataframe\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Clean state names by removing unusual characters\n",
    "    df_clean['State'] = df_clean['State'].str.replace('[^a-zA-Z\\s]', '', regex=True)\n",
    "    \n",
    "    # Melt the dataframe to convert years to rows\n",
    "    df_melted = pd.melt(\n",
    "        df_clean,\n",
    "        id_vars=['State'],\n",
    "        var_name='Year',\n",
    "        value_name='Lyme_cases'\n",
    "    )\n",
    "    \n",
    "    # Create region mapping\n",
    "    region_mapping = create_region_mapping()\n",
    "    \n",
    "    # Add region column\n",
    "    df_melted['region'] = df_melted['State'].map(region_mapping)\n",
    "    \n",
    "    # Convert Year to integer\n",
    "    df_melted['Year'] = pd.to_numeric(df_melted['Year'])\n",
    "    \n",
    "    # Sort by region, state, and year\n",
    "    df_melted = df_melted.sort_values(['region', 'State', 'Year'])\n",
    "\n",
    "    df_melted['Lyme_cases'] = df_melted['Lyme_cases'].str.replace(',', '').astype(int)\n",
    "    \n",
    "    return df_melted\n",
    "\n",
    "# Initialize df as None before the loop\n",
    "df = None\n",
    "\n",
    "# Try different encodings until one works\n",
    "encodings_to_try = ['latin1', 'cp1252', 'iso-8859-1', 'utf-8']\n",
    "for encoding in encodings_to_try:\n",
    "    try:\n",
    "        print(f\"Trying {encoding} encoding...\")\n",
    "        df = pd.read_csv('../data/raw_data/lyme_states_2008-2022_WIDE.csv', encoding=encoding)\n",
    "        print(f\"Successfully read file with {encoding} encoding\")\n",
    "        break\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Failed with {encoding} encoding\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"Different error with {encoding} encoding: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Check if we successfully loaded the data\n",
    "if df is None:\n",
    "    raise Exception(\"Could not read the CSV file with any of the attempted encodings\")\n",
    "\n",
    "# Clean the data\n",
    "state_lyme = clean_lyme_data(df)\n",
    "\n",
    "# Display first few rows and basic information\n",
    "print(\"\\nFirst few rows of the cleaned dataset:\")\n",
    "print(state_lyme.head())\n",
    "\n",
    "# Print unique state names to verify cleaning worked\n",
    "print(\"\\nUnique state names after cleaning:\")\n",
    "print(sorted(state_lyme['State'].unique()))\n",
    "\n",
    "print(\"\\nSummary of data by region:\")\n",
    "print(state_lyme.groupby('region')['State'].nunique().to_frame('Number of States'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Climate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing average temperature data...\n",
      "../data/raw_data/western_avgtemp.csv\n",
      "../data/raw_data/central_avgtemp.csv\n",
      "../data/raw_data/eastern_avgtemp.csv\n",
      "../data/raw_data/southern_avgtemp.csv\n",
      "\n",
      "Processing ../data/raw_data/western_avgtemp.csv\n",
      "Extracted region: western\n",
      "Read 21 rows from file\n",
      "Successfully processed ../data/raw_data/western_avgtemp.csv\n",
      "\n",
      "Processing ../data/raw_data/central_avgtemp.csv\n",
      "Extracted region: central\n",
      "Read 21 rows from file\n",
      "Successfully processed ../data/raw_data/central_avgtemp.csv\n",
      "\n",
      "Processing ../data/raw_data/eastern_avgtemp.csv\n",
      "Extracted region: eastern\n",
      "Read 21 rows from file\n",
      "Successfully processed ../data/raw_data/eastern_avgtemp.csv\n",
      "\n",
      "Processing ../data/raw_data/southern_avgtemp.csv\n",
      "Extracted region: southern\n",
      "Read 21 rows from file\n",
      "Successfully processed ../data/raw_data/southern_avgtemp.csv\n",
      "\n",
      "Processing minimum temperature data...\n",
      "\n",
      "Found these mintemp files:\n",
      "../data/raw_data/western_mintemp.csv\n",
      "../data/raw_data/southern_mintemp.csv\n",
      "../data/raw_data/eastern_mintemp.csv\n",
      "../data/raw_data/central_mintemp.csv\n",
      "\n",
      "Processing ../data/raw_data/western_mintemp.csv\n",
      "Extracted region: western\n",
      "Read 15 rows from file\n",
      "Successfully processed ../data/raw_data/western_mintemp.csv\n",
      "\n",
      "Processing ../data/raw_data/southern_mintemp.csv\n",
      "Extracted region: southern\n",
      "Read 15 rows from file\n",
      "Successfully processed ../data/raw_data/southern_mintemp.csv\n",
      "\n",
      "Processing ../data/raw_data/eastern_mintemp.csv\n",
      "Extracted region: eastern\n",
      "Read 15 rows from file\n",
      "Successfully processed ../data/raw_data/eastern_mintemp.csv\n",
      "\n",
      "Processing ../data/raw_data/central_mintemp.csv\n",
      "Extracted region: central\n",
      "Read 15 rows from file\n",
      "Successfully processed ../data/raw_data/central_mintemp.csv\n",
      "\n",
      "Processing processing data...\n",
      "../data/raw_data/western_precipitation.csv\n",
      "../data/raw_data/central_precipitation.csv\n",
      "../data/raw_data/eastern_precipitation.csv\n",
      "../data/raw_data/southern_precipitation.csv\n",
      "\n",
      "Processing ../data/raw_data/western_precipitation.csv\n",
      "Extracted region: western\n",
      "Read 15 rows from file\n",
      "Successfully processed ../data/raw_data/western_precipitation.csv\n",
      "\n",
      "Processing ../data/raw_data/central_precipitation.csv\n",
      "Extracted region: central\n",
      "Read 15 rows from file\n",
      "Successfully processed ../data/raw_data/central_precipitation.csv\n",
      "\n",
      "Processing ../data/raw_data/eastern_precipitation.csv\n",
      "Extracted region: eastern\n",
      "Read 15 rows from file\n",
      "Successfully processed ../data/raw_data/eastern_precipitation.csv\n",
      "\n",
      "Processing ../data/raw_data/southern_precipitation.csv\n",
      "Extracted region: southern\n",
      "Read 15 rows from file\n",
      "Successfully processed ../data/raw_data/southern_precipitation.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def clean_avgtemp_data():\n",
    "    \n",
    "    # List to store dataframes\n",
    "    avg_dfs = []\n",
    "    \n",
    "    # find all avgtemp data for each region\n",
    "    avgtemp_files = glob.glob(\"../data/raw_data/*avgtemp.csv\")  \n",
    "    for file in avgtemp_files:\n",
    "        print(file)\n",
    "    \n",
    "    # Process each avgtemp file for the regions\n",
    "    for file_path in avgtemp_files:\n",
    "        print(f\"\\nProcessing {file_path}\")\n",
    "        # Extract region from filename\n",
    "        region = os.path.basename(file_path).split('_')[0]\n",
    "        print(f\"Extracted region: {region}\")\n",
    "        \n",
    "        try:\n",
    "            # Read CSV file, skipping the first 4 rows\n",
    "            df = pd.read_csv(file_path, skiprows=4)\n",
    "            print(f\"Read {len(df)} rows from file\")\n",
    "            \n",
    "            # Clean and rename columns\n",
    "            df = df[['Date', 'Value']]  # Keep only needed columns\n",
    "            df = df.rename(columns={\n",
    "                'Date': 'Year',\n",
    "                'Value': 'Avg_temp'\n",
    "            })\n",
    "            \n",
    "            # Extract year from the date column\n",
    "            df['Year'] = df['Year'].astype(str).str[:4].astype(int)\n",
    "            \n",
    "            # Add region column\n",
    "            df['region'] = region\n",
    "            \n",
    "            avg_dfs.append(df)\n",
    "            print(f\"Successfully processed {file_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "    \n",
    "    # Combine all regions\n",
    "    if avg_dfs:\n",
    "        return pd.concat(avg_dfs, ignore_index=True)\n",
    "    return None\n",
    "\n",
    "def clean_mintemp_data():\n",
    "    # List to store dataframes\n",
    "    min_dfs = []\n",
    "    \n",
    "    # Show which files we're finding\n",
    "    print(\"\\nFound these mintemp files:\")\n",
    "    mintemp_files = glob.glob(\"../data/raw_data/*mintemp.csv\")  # Removed underscore from pattern\n",
    "    for file in mintemp_files:\n",
    "        print(file)\n",
    "    \n",
    "    # Process each mintemp file for the regions\n",
    "    for file_path in mintemp_files:\n",
    "        print(f\"\\nProcessing {file_path}\")\n",
    "        # Extract region from filename\n",
    "        region = os.path.basename(file_path).split('_')[0]\n",
    "        print(f\"Extracted region: {region}\")\n",
    "        \n",
    "        try:\n",
    "            # Read CSV file, skipping the first 4 rows\n",
    "            df = pd.read_csv(file_path, skiprows=4)\n",
    "            print(f\"Read {len(df)} rows from file\")\n",
    "            \n",
    "            # Clean and rename columns\n",
    "            df = df[['Date', 'Value']]  # Keep only needed columns\n",
    "            df = df.rename(columns={\n",
    "                'Date': 'Year',\n",
    "                'Value': 'Min_temp_avg'\n",
    "            })\n",
    "            \n",
    "            # Extract year from the date column\n",
    "            df['Year'] = df['Year'].astype(str).str[:4].astype(int)\n",
    "            \n",
    "            # Add region column\n",
    "            df['region'] = region\n",
    "            \n",
    "            min_dfs.append(df)\n",
    "            print(f\"Successfully processed {file_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "    \n",
    "    # Combine all regions\n",
    "    if min_dfs:\n",
    "        return pd.concat(min_dfs, ignore_index=True)\n",
    "    return None\n",
    "\n",
    "\n",
    "# PRECIPITATION DATA\n",
    "def clean_precipitation_data():\n",
    "    # List to store dataframes\n",
    "    min_dfs = []\n",
    "    \n",
    "    # Show which files we're finding\n",
    "    precipitation_files = glob.glob(\"../data/raw_data/*precipitation.csv\")  # Removed underscore from pattern\n",
    "    for file in precipitation_files:\n",
    "        print(file)\n",
    "    \n",
    "    # Process each mintemp file for the regions\n",
    "    for file_path in precipitation_files:\n",
    "        print(f\"\\nProcessing {file_path}\")\n",
    "        # Extract region from filename\n",
    "        region = os.path.basename(file_path).split('_')[0]\n",
    "        print(f\"Extracted region: {region}\")\n",
    "        \n",
    "        try:\n",
    "            # Read CSV file, skipping the first 4 rows\n",
    "            df = pd.read_csv(file_path, skiprows=4)\n",
    "            print(f\"Read {len(df)} rows from file\")\n",
    "            \n",
    "            # Clean and rename columns\n",
    "            df = df[['Date', 'Value']]  # Keep only needed columns\n",
    "            df = df.rename(columns={\n",
    "                'Date': 'Year',\n",
    "                'Value': 'Precipitation_avg'\n",
    "            })\n",
    "            \n",
    "            # Extract year from the date column\n",
    "            df['Year'] = df['Year'].astype(str).str[:4].astype(int)\n",
    "            \n",
    "            # Add region column\n",
    "            df['region'] = region\n",
    "            \n",
    "            min_dfs.append(df)\n",
    "            print(f\"Successfully processed {file_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "    \n",
    "    # Combine all regions\n",
    "    if min_dfs:\n",
    "        return pd.concat(min_dfs, ignore_index=True)\n",
    "    return None\n",
    "\n",
    "# Process both types of data\n",
    "print(\"Processing average temperature data...\")\n",
    "avg_temp_data = clean_avgtemp_data()\n",
    "print(\"\\nProcessing minimum temperature data...\")\n",
    "min_temp_data = clean_mintemp_data()\n",
    "print(\"\\nProcessing processing data...\")\n",
    "precipitation_data = clean_precipitation_data()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Merge average and minimum temperature data\n",
    "# if avg_temp_data is not None and min_temp_data is not None:\n",
    "#     final_data = min_temp_data.merge(avg_temp_data, on=['year', 'region'], how='left')\n",
    "    \n",
    "#     # Sort by region and year\n",
    "#     final_data = final_data.sort_values(['region', 'year'])\n",
    "    \n",
    "#     # Save the merged dataset\n",
    "#     final_data.to_csv(\"merged_temperature_data.csv\", index=False)\n",
    "    \n",
    "#     print(\"\\nData processing completed successfully!\")\n",
    "#     print(\"\\nFirst few rows of the merged dataset:\")\n",
    "#     print(final_data.head())\n",
    "# else:\n",
    "#     print(\"Error: No data was processed. Please check if the input files exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree coverage loss data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        State  Total_Land_Area  Year  Tree_Cover_Loss\n",
      "0     Alabama         13363464  2001           168587\n",
      "1      Alaska        150737804  2001            27964\n",
      "2     Arizona         29535713  2001              653\n",
      "3    Arkansas         13769059  2001           110114\n",
      "4  California         40961694  2001            39102\n"
     ]
    }
   ],
   "source": [
    "tcloss = pd.read_csv('../data/raw_data/treecoverlossdata.csv')\n",
    "\n",
    "# 1. Filter for threshold = 75 and remove the threshold column\n",
    "filtered_data = tcloss[tcloss['threshold'] == 75]\n",
    "\n",
    "# 2. Remove unnecessary columns\n",
    "columns_to_remove = ['country', 'extent_2000_ha', 'extent_2010_ha', 'gain_2000-2020_ha', 'threshold']\n",
    "filtered_data = filtered_data.drop(columns=columns_to_remove)\n",
    "\n",
    "# 3. Melt on tc_loss_ha_20** columns\n",
    "melted_tc = pd.melt(\n",
    "    filtered_data,\n",
    "    id_vars=['subnational1', 'area_ha'],\n",
    "    value_vars=[col for col in filtered_data.columns if col.startswith('tc_loss_ha_')],\n",
    "    var_name='Year',\n",
    "    value_name='Tree_Cover_Loss'\n",
    ")\n",
    "\n",
    "# 4. Clean the 'Year' column to retain only the numeric year\n",
    "melted_tc['Year'] = melted_tc['Year'].str.extract(r'(\\d{4})')\n",
    "melted_tc['Year'] = pd.to_numeric(melted_tc['Year'])\n",
    "\n",
    "# 5. Rename columns\n",
    "melted_tc = melted_tc.rename(columns={'subnational1': 'State', 'area_ha': 'Total_Land_Area'})\n",
    "\n",
    "# Display the cleaned data\n",
    "print(melted_tc.head())  # Replace with tools to display if needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Species richness data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        State  species_richness  state_park_land_coverage  state_park_rank\n",
      "0     Alabama              10.0                    0.0023               46\n",
      "1      Alaska               NaN                    0.0910                2\n",
      "2     Arizona               5.0                    0.0260               12\n",
      "3    Arkansas               8.0                    0.0018               44\n",
      "4  California              10.0                    0.0749                3\n"
     ]
    }
   ],
   "source": [
    "species = pd.read_csv('../data/raw_data/species_richness_by_state.csv')\n",
    "species = species.rename(columns={'state': 'State'})\n",
    "species['state_park_land_coverage'] = species['state_park_land_coverage'].str.replace('%', '')\n",
    "species['state_park_land_coverage'] = pd.to_numeric(species['state_park_land_coverage']) / 100\n",
    "print(species.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year  Precipitation_avg   region  Min_temp_avg  Avg_temp     State  \\\n",
      "0  2008               4.49  central          53.8      66.1  Illinois   \n",
      "1  2008               4.49  central          53.8      66.1   Indiana   \n",
      "2  2008               4.49  central          53.8      66.1      Iowa   \n",
      "3  2008               4.49  central          53.8      66.1    Kansas   \n",
      "4  2008               4.49  central          53.8      66.1  Michigan   \n",
      "\n",
      "   Lyme_cases  Total_Land_Area  Tree_Cover_Loss  species_richness  \\\n",
      "0         108         15008781              615               6.0   \n",
      "1          42          9436269             1051               8.0   \n",
      "2         109         14584483              209               5.0   \n",
      "3          16         21312413              460               1.0   \n",
      "4          92         25036039            27919               3.0   \n",
      "\n",
      "   state_park_land_coverage  state_park_rank  \n",
      "0                    0.0139             19.0  \n",
      "1                    0.0081             32.0  \n",
      "2                    0.0021             42.0  \n",
      "3                    0.0006             44.0  \n",
      "4                    0.0223             11.0  \n"
     ]
    }
   ],
   "source": [
    "merged = min_temp_data.merge(avg_temp_data, on=['Year', 'region'], how='left')\n",
    "merged2 = precipitation_data.merge(merged, on=['Year', 'region'], how='outer')\n",
    "merged3 = merged2.merge(state_lyme, on=['Year', 'region'], how='left')\n",
    "merged4 = merged3.merge(melted_tc, on=['Year', 'State'], how=\"left\")\n",
    "merged5 = merged4.merge(species, on='State', how='left')\n",
    "print(merged5.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Year                          int64\n",
       "Precipitation_avg           float64\n",
       "region                       object\n",
       "Min_temp_avg                float64\n",
       "Avg_temp                    float64\n",
       "State                        object\n",
       "Lyme_cases                    int64\n",
       "Total_Land_Area               int64\n",
       "Tree_Cover_Loss               int64\n",
       "species_richness            float64\n",
       "state_park_land_coverage    float64\n",
       "state_park_rank             float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged5.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged5.to_csv('../data/clean_data/data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning RCP 8.5 future climate scenarios data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error processing file ../data/raw_data/iowa_central_rcp8.5.csv:\n",
      "Error message: 'Scenario 4'\n",
      "File: iowa\n",
      "DataFrame shape: (9, 1)\n",
      "DataFrame columns: ['********************************************************']\n",
      "First few rows of data:\n",
      "  ********************************************************\n",
      "0  Climate Metric, Scenario 1, Scenario 2, Scenar...      \n",
      "1  Summer Mean Temperature(°F),73.602249,77.45282...      \n",
      "2  (change relative to historical by °F),1.06,4.9...      \n",
      "3  Summer Minimum Temperature(°F),62.734554,65.90...      \n",
      "4  (change relative to historical by °F),1.26,4.4...      \n",
      "\n",
      "Error processing file ../data/raw_data/mississippi-2050_rcp8.5.csv:\n",
      "Error message: 'Scenario 4'\n",
      "File: mississippi-2050\n",
      "DataFrame shape: (9, 1)\n",
      "DataFrame columns: ['********************************************************']\n",
      "First few rows of data:\n",
      "  ********************************************************\n",
      "0  Climate Metric, Scenario 1, Scenario 2, Scenar...      \n",
      "1  Summer Mean Temperature(°F),84.072647,84.74151...      \n",
      "2  (change relative to historical by °F),3.62,4.2...      \n",
      "3  Summer Minimum Temperature(°F),72.849205,73.81...      \n",
      "4  (change relative to historical by °F),3.20,4.1...      \n",
      "\n",
      "Error processing file ../data/raw_data/westernUS_rcp8.5.csv:\n",
      "Error message: 'Scenario 4'\n",
      "File: westernUS\n",
      "DataFrame shape: (9, 1)\n",
      "DataFrame columns: ['********************************************************']\n",
      "First few rows of data:\n",
      "  ********************************************************\n",
      "0  Climate Metric, Scenario 1, Scenario 2, Scenar...      \n",
      "1  Summer Mean Temperature(°F),70.447868,72.80569...      \n",
      "2  (change relative to historical by °F),2.36,4.7...      \n",
      "3  Summer Minimum Temperature(°F),54.665958,57.36...      \n",
      "4  (change relative to historical by °F),1.73,4.4...      \n",
      "\n",
      "Error processing file ../data/raw_data/northernUS_rcp8.5.csv:\n",
      "Error message: 'Scenario 4'\n",
      "File: northernUS\n",
      "DataFrame shape: (9, 1)\n",
      "DataFrame columns: ['********************************************************']\n",
      "First few rows of data:\n",
      "  ********************************************************\n",
      "0  Climate Metric, Scenario 1, Scenario 2, Scenar...      \n",
      "1  Summer Mean Temperature(°F),68.735367,70.57825...      \n",
      "2  (change relative to historical by °F),2.62,4.4...      \n",
      "3  Summer Minimum Temperature(°F),57.713863,59.44...      \n",
      "4  (change relative to historical by °F),2.55,4.2...      \n",
      "\n",
      "No data was successfully processed.\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# def process_climate_file(file_path):\n",
    "#     # Define mapping for filename to region and number of rows to skip\n",
    "#     file_configs = {\n",
    "#         'westernUS': {'region': 'western', 'skiprows': 13},\n",
    "#         'mississippi-2050': {'region': 'southern', 'skiprows': 11},\n",
    "#         'northernUS': {'region': 'northern', 'skiprows': 13},\n",
    "#         'iowa': {'region': 'central', 'skiprows': 11}  # Fixed filename for iowa\n",
    "#     }\n",
    "    \n",
    "#     # Extract filename base (handle both iowa and iowa_central cases)\n",
    "#     file_name = os.path.basename(file_path).split('_')[0]\n",
    "    \n",
    "#     # Get configuration for this file\n",
    "#     config = file_configs.get(file_name)\n",
    "#     if not config:\n",
    "#         print(f\"Unknown file format: {file_name}\")\n",
    "#         return None\n",
    "    \n",
    "#     try:\n",
    "#         # Read CSV file with appropriate number of rows to skip and tab separator\n",
    "#         df = pd.read_csv(file_path, skiprows=config['skiprows'] - 1, sep='\\t')\n",
    "        \n",
    "#         # Extract the specific values from Scenario 4 column\n",
    "#         climate_data = {\n",
    "#             'Indicator': ['Avg_temp', 'Min_temp', 'Precipitation_avg'],\n",
    "#             'value': [\n",
    "#                 float(df.iloc[1]['Scenario 4'].strip()),    # Summer Mean Temperature\n",
    "#                 float(df.iloc[3]['Scenario 4'].strip()),    # Summer Minimum Temperature\n",
    "#                 float(df.iloc[5]['Scenario 4'].strip())     # Summer Precipitation\n",
    "#             ],\n",
    "#             'region': [config['region']] * 3  # Use the region from config\n",
    "#         }\n",
    "        \n",
    "#         # Create DataFrame from extracted data\n",
    "#         result_df = pd.DataFrame(climate_data)\n",
    "        \n",
    "#         # Print debug information\n",
    "#         print(f\"\\nProcessing {file_name}:\")\n",
    "#         print(f\"Avg_temp: {climate_data['value'][0]}\")\n",
    "#         print(f\"Min_temp: {climate_data['value'][1]}\")\n",
    "#         print(f\"Precipitation_avg: {climate_data['value'][2]}\")\n",
    "        \n",
    "#         return result_df\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"\\nError processing file {file_path}:\")\n",
    "#         print(f\"Error message: {str(e)}\")\n",
    "#         print(f\"File: {file_name}\")\n",
    "#         if 'df' in locals():\n",
    "#             print(f\"DataFrame shape: {df.shape}\")\n",
    "#             print(f\"DataFrame columns: {df.columns.tolist()}\")\n",
    "#             print(f\"First few rows of data:\")\n",
    "#             print(df.head())\n",
    "#         return None\n",
    "\n",
    "# def process_all_climate_files(directory_path):\n",
    "#     all_dfs = []\n",
    "    \n",
    "#     # Find all relevant CSV files\n",
    "#     for file in os.listdir(directory_path):\n",
    "#         if file.endswith('rcp8.5.csv'):\n",
    "#             file_path = os.path.join(directory_path, file)\n",
    "#             processed_df = process_climate_file(file_path)\n",
    "#             if processed_df is not None:\n",
    "#                 all_dfs.append(processed_df)\n",
    "    \n",
    "#     # Combine all DataFrames if we have any\n",
    "#     if all_dfs:\n",
    "#         final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        \n",
    "#         # Ensure numeric values in 'value' column\n",
    "#         final_df['value'] = pd.to_numeric(final_df['value'], errors='coerce')\n",
    "        \n",
    "#         return final_df\n",
    "#     else:\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "# # Example usage:\n",
    "# if __name__ == \"__main__\":\n",
    "#     directory_path = \"../data/raw_data\"  # Your directory path\n",
    "    \n",
    "#     # Process all files\n",
    "#     combined_climate_data = process_all_climate_files(directory_path)\n",
    "    \n",
    "#     if not combined_climate_data.empty:\n",
    "#         print(\"\\nFinal Combined Climate Data:\")\n",
    "#         print(combined_climate_data)\n",
    "        \n",
    "#         # Save to CSV\n",
    "#         output_path = \"processed_climate_data.csv\"\n",
    "#         combined_climate_data.to_csv(output_path, index=False)\n",
    "#         print(f\"\\nData saved to {output_path}\")\n",
    "#     else:\n",
    "#         print(\"\\nNo data was successfully processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Indicator  value    region  Total_Land_Area  \\\n",
      "0            Avg_temp  86.36  southern       2105379510   \n",
      "1            Min_temp  75.29  southern       2105379510   \n",
      "2   Precipitation_avg  12.58  southern       2105379510   \n",
      "3            Avg_temp  72.39   eastern        710057100   \n",
      "4            Min_temp  61.21   eastern        710057100   \n",
      "5   Precipitation_avg  12.76   eastern        710057100   \n",
      "6            Avg_temp  79.33   central       3198039870   \n",
      "7            Min_temp  67.81   central       3198039870   \n",
      "8   Precipitation_avg  12.41   central       3198039870   \n",
      "9            Avg_temp  74.49   western       6895077150   \n",
      "10           Min_temp  59.00   western       6895077150   \n",
      "11  Precipitation_avg   7.50   western       6895077150   \n",
      "\n",
      "    state_park_land_coverage  state_park_rank  \n",
      "0                   0.012292        30.916667  \n",
      "1                   0.012292        30.916667  \n",
      "2                   0.012292        30.916667  \n",
      "3                   0.027827        17.363636  \n",
      "4                   0.027827        17.363636  \n",
      "5                   0.027827        17.363636  \n",
      "6                   0.008292        31.083333  \n",
      "7                   0.008292        31.083333  \n",
      "8                   0.008292        31.083333  \n",
      "9                   0.032108        19.923077  \n",
      "10                  0.032108        19.923077  \n",
      "11                  0.032108        19.923077  \n"
     ]
    }
   ],
   "source": [
    "rcp = pd.read_csv('../data/clean_data/RCP8.5_data.csv')\n",
    "\n",
    "region_aggregates = merged5.groupby('region').agg({\n",
    "    'Total_Land_Area': 'sum',  # Sum for total land area\n",
    "    'state_park_land_coverage': 'mean',  # Average for park land coverage\n",
    "    'state_park_rank': 'mean'  # Average for park rank\n",
    "}).reset_index()\n",
    "\n",
    "# Merge region-level climate data with the aggregated state-level data\n",
    "# Assuming `climate_df` contains your regional climate data for future years\n",
    "# Example: {'region': ['central', 'eastern', ...], 'Precipitation_avg': [value1, value2, ...]}\n",
    "future_data = pd.merge(rcp, region_aggregates, on='region')\n",
    "\n",
    "print(future_data)\n",
    "\n",
    "future_data.to_csv('../data/clean_data/2050inputs.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
