{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:70: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:70: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/var/folders/0s/3s0t3s4d31d4xtm_jt4pfhpr0000gn/T/ipykernel_25489/967449051.py:70: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  df_clean['State'] = df_clean['State'].str.replace('[^a-zA-Z\\s]', '', regex=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying latin1 encoding...\n",
      "Successfully read file with latin1 encoding\n",
      "\n",
      "First few rows of the cleaned dataset:\n",
      "        State  Year  Lyme_cases   region\n",
      "13   Illinois  2008         108  central\n",
      "65   Illinois  2009         136  central\n",
      "117  Illinois  2010         135  central\n",
      "169  Illinois  2011         194  central\n",
      "221  Illinois  2012         204  central\n",
      "\n",
      "Unique state names after cleaning:\n",
      "['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware', 'District of Columbia', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'US Total', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming']\n",
      "\n",
      "Summary of data by region:\n",
      "          Number of States\n",
      "region                    \n",
      "central                 12\n",
      "eastern                 12\n",
      "southern                12\n",
      "western                 13\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_region_mapping():\n",
    "    \"\"\"Create dictionary mapping states to their regions\"\"\"\n",
    "    return {\n",
    "        # [Your existing region mapping dictionary stays the same]\n",
    "        # Northeast/Eastern Region\n",
    "        'Maine': 'eastern',\n",
    "        'New Hampshire': 'eastern',\n",
    "        'Vermont': 'eastern',\n",
    "        'Massachusetts': 'eastern',\n",
    "        'Rhode Island': 'eastern',\n",
    "        'Connecticut': 'eastern',\n",
    "        'New York': 'eastern',\n",
    "        'Pennsylvania': 'eastern',\n",
    "        'New Jersey': 'eastern',\n",
    "        'Delaware': 'eastern',\n",
    "        'Maryland': 'eastern',\n",
    "        'District of Columbia': 'eastern',\n",
    "        \n",
    "        # Southern Region\n",
    "        'Virginia': 'southern',\n",
    "        'West Virginia': 'southern',\n",
    "        'Kentucky': 'southern',\n",
    "        'Tennessee': 'southern',\n",
    "        'North Carolina': 'southern',\n",
    "        'South Carolina': 'southern',\n",
    "        'Georgia': 'southern',\n",
    "        'Florida': 'southern',\n",
    "        'Alabama': 'southern',\n",
    "        'Mississippi': 'southern',\n",
    "        'Louisiana': 'southern',\n",
    "        'Arkansas': 'southern',\n",
    "        \n",
    "        # Central Region\n",
    "        'Ohio': 'central',\n",
    "        'Indiana': 'central',\n",
    "        'Illinois': 'central',\n",
    "        'Michigan': 'central',\n",
    "        'Wisconsin': 'central',\n",
    "        'Minnesota': 'central',\n",
    "        'Iowa': 'central',\n",
    "        'Missouri': 'central',\n",
    "        'North Dakota': 'central',\n",
    "        'South Dakota': 'central',\n",
    "        'Nebraska': 'central',\n",
    "        'Kansas': 'central',\n",
    "        \n",
    "        # Western Region\n",
    "        'Montana': 'western',\n",
    "        'Idaho': 'western',\n",
    "        'Wyoming': 'western',\n",
    "        'Colorado': 'western',\n",
    "        'New Mexico': 'western',\n",
    "        'Arizona': 'western',\n",
    "        'Utah': 'western',\n",
    "        'Nevada': 'western',\n",
    "        'California': 'western',\n",
    "        'Oregon': 'western',\n",
    "        'Washington': 'western',\n",
    "        'Alaska': 'western',\n",
    "        'Hawaii': 'western'\n",
    "    }\n",
    "\n",
    "def clean_lyme_data(df):\n",
    "    # Create copy of dataframe\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Clean state names by removing unusual characters\n",
    "    df_clean['State'] = df_clean['State'].str.replace('[^a-zA-Z\\s]', '', regex=True)\n",
    "    \n",
    "    # Melt the dataframe to convert years to rows\n",
    "    df_melted = pd.melt(\n",
    "        df_clean,\n",
    "        id_vars=['State'],\n",
    "        var_name='Year',\n",
    "        value_name='Lyme_cases'\n",
    "    )\n",
    "    \n",
    "    # Create region mapping\n",
    "    region_mapping = create_region_mapping()\n",
    "    \n",
    "    # Add region column\n",
    "    df_melted['region'] = df_melted['State'].map(region_mapping)\n",
    "    \n",
    "    # Convert Year to integer\n",
    "    df_melted['Year'] = pd.to_numeric(df_melted['Year'])\n",
    "    \n",
    "    # Sort by region, state, and year\n",
    "    df_melted = df_melted.sort_values(['region', 'State', 'Year'])\n",
    "\n",
    "    df_melted['Lyme_cases'] = df_melted['Lyme_cases'].str.replace(',', '').astype(int)\n",
    "    \n",
    "    return df_melted\n",
    "\n",
    "# Initialize df as None before the loop\n",
    "df = None\n",
    "\n",
    "# Try different encodings until one works\n",
    "encodings_to_try = ['latin1', 'cp1252', 'iso-8859-1', 'utf-8']\n",
    "for encoding in encodings_to_try:\n",
    "    try:\n",
    "        print(f\"Trying {encoding} encoding...\")\n",
    "        df = pd.read_csv('../data/raw_data/lyme_states_2008-2022_WIDE.csv', encoding=encoding)\n",
    "        print(f\"Successfully read file with {encoding} encoding\")\n",
    "        break\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Failed with {encoding} encoding\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"Different error with {encoding} encoding: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Check if we successfully loaded the data\n",
    "if df is None:\n",
    "    raise Exception(\"Could not read the CSV file with any of the attempted encodings\")\n",
    "\n",
    "# Clean the data\n",
    "state_lyme = clean_lyme_data(df)\n",
    "\n",
    "# Display first few rows and basic information\n",
    "print(\"\\nFirst few rows of the cleaned dataset:\")\n",
    "print(state_lyme.head())\n",
    "\n",
    "# Print unique state names to verify cleaning worked\n",
    "print(\"\\nUnique state names after cleaning:\")\n",
    "print(sorted(state_lyme['State'].unique()))\n",
    "\n",
    "print(\"\\nSummary of data by region:\")\n",
    "print(state_lyme.groupby('region')['State'].nunique().to_frame('Number of States'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing average temperature data...\n",
      "../data/raw_data/western_avgtemp.csv\n",
      "../data/raw_data/central_avgtemp.csv\n",
      "../data/raw_data/eastern_avgtemp.csv\n",
      "../data/raw_data/southern_avgtemp.csv\n",
      "\n",
      "Processing ../data/raw_data/western_avgtemp.csv\n",
      "Extracted region: western\n",
      "Read 21 rows from file\n",
      "Successfully processed ../data/raw_data/western_avgtemp.csv\n",
      "\n",
      "Processing ../data/raw_data/central_avgtemp.csv\n",
      "Extracted region: central\n",
      "Read 21 rows from file\n",
      "Successfully processed ../data/raw_data/central_avgtemp.csv\n",
      "\n",
      "Processing ../data/raw_data/eastern_avgtemp.csv\n",
      "Extracted region: eastern\n",
      "Read 21 rows from file\n",
      "Successfully processed ../data/raw_data/eastern_avgtemp.csv\n",
      "\n",
      "Processing ../data/raw_data/southern_avgtemp.csv\n",
      "Extracted region: southern\n",
      "Read 21 rows from file\n",
      "Successfully processed ../data/raw_data/southern_avgtemp.csv\n",
      "\n",
      "Processing minimum temperature data...\n",
      "\n",
      "Found these mintemp files:\n",
      "../data/raw_data/western_mintemp.csv\n",
      "../data/raw_data/southern_mintemp.csv\n",
      "../data/raw_data/eastern_mintemp.csv\n",
      "../data/raw_data/central_mintemp.csv\n",
      "\n",
      "Processing ../data/raw_data/western_mintemp.csv\n",
      "Extracted region: western\n",
      "Read 15 rows from file\n",
      "Successfully processed ../data/raw_data/western_mintemp.csv\n",
      "\n",
      "Processing ../data/raw_data/southern_mintemp.csv\n",
      "Extracted region: southern\n",
      "Read 15 rows from file\n",
      "Successfully processed ../data/raw_data/southern_mintemp.csv\n",
      "\n",
      "Processing ../data/raw_data/eastern_mintemp.csv\n",
      "Extracted region: eastern\n",
      "Read 15 rows from file\n",
      "Successfully processed ../data/raw_data/eastern_mintemp.csv\n",
      "\n",
      "Processing ../data/raw_data/central_mintemp.csv\n",
      "Extracted region: central\n",
      "Read 15 rows from file\n",
      "Successfully processed ../data/raw_data/central_mintemp.csv\n",
      "\n",
      "Processing precipitation data...\n",
      "../data/raw_data/western_precipitation.csv\n",
      "../data/raw_data/central_precipitation.csv\n",
      "../data/raw_data/eastern_precipitation.csv\n",
      "../data/raw_data/southern_precipitation.csv\n",
      "\n",
      "Processing ../data/raw_data/western_precipitation.csv\n",
      "Extracted region: western\n",
      "Read 15 rows from file\n",
      "Successfully processed ../data/raw_data/western_precipitation.csv\n",
      "\n",
      "Processing ../data/raw_data/central_precipitation.csv\n",
      "Extracted region: central\n",
      "Read 15 rows from file\n",
      "Successfully processed ../data/raw_data/central_precipitation.csv\n",
      "\n",
      "Processing ../data/raw_data/eastern_precipitation.csv\n",
      "Extracted region: eastern\n",
      "Read 15 rows from file\n",
      "Successfully processed ../data/raw_data/eastern_precipitation.csv\n",
      "\n",
      "Processing ../data/raw_data/southern_precipitation.csv\n",
      "Extracted region: southern\n",
      "Read 15 rows from file\n",
      "Successfully processed ../data/raw_data/southern_precipitation.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def clean_avgtemp_data():\n",
    "    \n",
    "    # List to store dataframes\n",
    "    avg_dfs = []\n",
    "    \n",
    "    # find all avgtemp data for each region\n",
    "    avgtemp_files = glob.glob(\"../data/raw_data/*avgtemp.csv\")  \n",
    "    for file in avgtemp_files:\n",
    "        print(file)\n",
    "    \n",
    "    # Process each avgtemp file for the regions\n",
    "    for file_path in avgtemp_files:\n",
    "        print(f\"\\nProcessing {file_path}\")\n",
    "        # Extract region from filename\n",
    "        region = os.path.basename(file_path).split('_')[0]\n",
    "        print(f\"Extracted region: {region}\")\n",
    "        \n",
    "        try:\n",
    "            # Read CSV file, skipping the first 4 rows\n",
    "            df = pd.read_csv(file_path, skiprows=4)\n",
    "            print(f\"Read {len(df)} rows from file\")\n",
    "            \n",
    "            # Clean and rename columns\n",
    "            df = df[['Date', 'Value']]  # Keep only needed columns\n",
    "            df = df.rename(columns={\n",
    "                'Date': 'Year',\n",
    "                'Value': 'Avg_temp'\n",
    "            })\n",
    "            \n",
    "            # Extract year from the date column\n",
    "            df['Year'] = df['Year'].astype(str).str[:4].astype(int)\n",
    "            \n",
    "            # Add region column\n",
    "            df['region'] = region\n",
    "            \n",
    "            avg_dfs.append(df)\n",
    "            print(f\"Successfully processed {file_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "    \n",
    "    # Combine all regions\n",
    "    if avg_dfs:\n",
    "        return pd.concat(avg_dfs, ignore_index=True)\n",
    "    return None\n",
    "\n",
    "def clean_mintemp_data():\n",
    "    # List to store dataframes\n",
    "    min_dfs = []\n",
    "    \n",
    "    # Show which files we're finding\n",
    "    print(\"\\nFound these mintemp files:\")\n",
    "    mintemp_files = glob.glob(\"../data/raw_data/*mintemp.csv\")  # Removed underscore from pattern\n",
    "    for file in mintemp_files:\n",
    "        print(file)\n",
    "    \n",
    "    # Process each mintemp file for the regions\n",
    "    for file_path in mintemp_files:\n",
    "        print(f\"\\nProcessing {file_path}\")\n",
    "        # Extract region from filename\n",
    "        region = os.path.basename(file_path).split('_')[0]\n",
    "        print(f\"Extracted region: {region}\")\n",
    "        \n",
    "        try:\n",
    "            # Read CSV file, skipping the first 4 rows\n",
    "            df = pd.read_csv(file_path, skiprows=4)\n",
    "            print(f\"Read {len(df)} rows from file\")\n",
    "            \n",
    "            # Clean and rename columns\n",
    "            df = df[['Date', 'Value']]  # Keep only needed columns\n",
    "            df = df.rename(columns={\n",
    "                'Date': 'Year',\n",
    "                'Value': 'Min_temp_avg'\n",
    "            })\n",
    "            \n",
    "            # Extract year from the date column\n",
    "            df['Year'] = df['Year'].astype(str).str[:4].astype(int)\n",
    "            \n",
    "            # Add region column\n",
    "            df['region'] = region\n",
    "            \n",
    "            min_dfs.append(df)\n",
    "            print(f\"Successfully processed {file_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "    \n",
    "    # Combine all regions\n",
    "    if min_dfs:\n",
    "        return pd.concat(min_dfs, ignore_index=True)\n",
    "    return None\n",
    "\n",
    "\n",
    "# PRECIPITATION DATA\n",
    "def clean_precipitation_data():\n",
    "    # List to store dataframes\n",
    "    min_dfs = []\n",
    "    \n",
    "    # Show which files we're finding\n",
    "    precipitation_files = glob.glob(\"../data/raw_data/*precipitation.csv\")  # Removed underscore from pattern\n",
    "    for file in precipitation_files:\n",
    "        print(file)\n",
    "    \n",
    "    # Process each mintemp file for the regions\n",
    "    for file_path in precipitation_files:\n",
    "        print(f\"\\nProcessing {file_path}\")\n",
    "        # Extract region from filename\n",
    "        region = os.path.basename(file_path).split('_')[0]\n",
    "        print(f\"Extracted region: {region}\")\n",
    "        \n",
    "        try:\n",
    "            # Read CSV file, skipping the first 4 rows\n",
    "            df = pd.read_csv(file_path, skiprows=4)\n",
    "            print(f\"Read {len(df)} rows from file\")\n",
    "            \n",
    "            # Clean and rename columns\n",
    "            df = df[['Date', 'Value']]  # Keep only needed columns\n",
    "            df = df.rename(columns={\n",
    "                'Date': 'Year',\n",
    "                'Value': 'Precipitation_avg'\n",
    "            })\n",
    "            \n",
    "            # Extract year from the date column\n",
    "            df['Year'] = df['Year'].astype(str).str[:4].astype(int)\n",
    "            \n",
    "            # Add region column\n",
    "            df['region'] = region\n",
    "            \n",
    "            min_dfs.append(df)\n",
    "            print(f\"Successfully processed {file_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "    \n",
    "    # Combine all regions\n",
    "    if min_dfs:\n",
    "        return pd.concat(min_dfs, ignore_index=True)\n",
    "    return None\n",
    "\n",
    "# Process both types of data\n",
    "print(\"Processing average temperature data...\")\n",
    "avg_temp_data = clean_avgtemp_data()\n",
    "print(\"\\nProcessing minimum temperature data...\")\n",
    "min_temp_data = clean_mintemp_data()\n",
    "print(\"\\nProcessing precipitation data...\")\n",
    "precipitation_data = clean_precipitation_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree coverage loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        State  Total_Land_Area  Year  Tree_Cover_Loss\n",
      "0     Alabama         13363464  2001           168587\n",
      "1      Alaska        150737804  2001            27964\n",
      "2     Arizona         29535713  2001              653\n",
      "3    Arkansas         13769059  2001           110114\n",
      "4  California         40961694  2001            39102\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tcloss= pd.read_csv('../data/raw_data/treecoverlossdata.csv')\n",
    "\n",
    "# 1. Filter for threshold = 75 and remove the threshold column\n",
    "filtered_data = tcloss[tcloss['threshold'] == 75].drop(columns=['threshold'])\n",
    "\n",
    "# 2. Remove unnecessary columns\n",
    "columns_to_remove = ['country', 'extent_2000_ha', 'extent_2010_ha', 'gain_2000-2020_ha']\n",
    "filtered_data = filtered_data.drop(columns=columns_to_remove)\n",
    "\n",
    "# 3. Melt on tc_loss_ha_20** columns\n",
    "tc_melted = pd.melt(\n",
    "    filtered_data,\n",
    "    id_vars=['subnational1', 'area_ha'],\n",
    "    value_vars=[col for col in filtered_data.columns if col.startswith('tc_loss_ha_')],\n",
    "    var_name='Year',\n",
    "    value_name='Tree_Cover_Loss'\n",
    ")\n",
    "\n",
    "# 4. Clean the 'Year' column to retain only the numeric year\n",
    "tc_melted['Year'] = tc_melted['Year'].str.extract(r'(\\d{4})')\n",
    "tc_melted['Year'] = pd.to_numeric(tc_melted['Year'])\n",
    "\n",
    "# 5. Rename columns\n",
    "tc_melted = tc_melted.rename(columns={'subnational1': 'State', 'area_ha': 'Total_Land_Area'})\n",
    "\n",
    "# Display the cleaned data\n",
    "print(tc_melted.head())  # Replace with tools to display if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Species richness data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        State  species_richness  state_park_land_coverage  state_park_rank\n",
      "0     Alabama              10.0                    0.0023               46\n",
      "1      Alaska               NaN                    0.0910                2\n",
      "2     Arizona               5.0                    0.0260               12\n",
      "3    Arkansas               8.0                    0.0018               44\n",
      "4  California              10.0                    0.0749                3\n"
     ]
    }
   ],
   "source": [
    "species = pd.read_csv('../data/raw_data/species_richness_by_state.csv')\n",
    "species.rename(columns={'state': 'State'}, inplace=True)\n",
    "species['state_park_land_coverage'] = species['state_park_land_coverage'].str.replace('%', '')\n",
    "species['state_park_land_coverage'] = pd.to_numeric(species['state_park_land_coverage']) / 100\n",
    "print(species.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year                          int64\n",
      "Precipitation_avg           float64\n",
      "region                       object\n",
      "Min_temp_avg                float64\n",
      "Avg_temp                    float64\n",
      "State                        object\n",
      "Lyme_cases                    int64\n",
      "Total_Land_Area               int64\n",
      "Tree_Cover_Loss               int64\n",
      "species_richness            float64\n",
      "state_park_land_coverage    float64\n",
      "state_park_rank             float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "merged = min_temp_data.merge(avg_temp_data, on=['Year', 'region'], how='left')\n",
    "merged2 = precipitation_data.merge(merged, on=['Year', 'region'], how='outer')\n",
    "merged3 = merged2.merge(state_lyme, on=['Year', 'region'], how='left')\n",
    "merged4 = merged3.merge(tc_melted, on=['Year', 'State'], how='left')\n",
    "merged5 = merged4.merge(species, on='State', how='left')\n",
    "print(merged5.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged5.to_csv('../data/clean_data/state_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
